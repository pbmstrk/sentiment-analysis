{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag_of_Words_JAX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPF0ybOfmb9mjV56h6QkXrj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m-fAnSxWILE",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis: Bag-of-Words (JAX and Haiku)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEa-uGfWugY",
        "colab_type": "text"
      },
      "source": [
        "The notebook is roughly divided into two sections: pre-processing and the actual model.\n",
        "\n",
        "## Setup and downloading data\n",
        "\n",
        "We use the Kaggle API to download the data. \n",
        "\n",
        "**N.B.** If you want to run the notebook you need to upload a `kaggle.json` file which contains your API credentials. Instructions for downloading the file from Kaggle can be found [here](https://github.com/Kaggle/kaggle-api).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fit3D94Gx29r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the kaggle API and haiku\n",
        "!pip install kaggle -q\n",
        "!pip install dm-haiku -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG9aRGVOx8dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data wrangling\n",
        "import numpy as onp\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# language model \n",
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "# jax and haiku\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from jax.experimental import optix\n",
        "\n",
        "# data pipeline\n",
        "from torch.utils import data\n",
        "\n",
        "from collections import Counter\n",
        "import copy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4UIHccFygj8",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7d52177c-9001-47c4-d0e4-d75e707563da"
      },
      "source": [
        "# upload kaggle.json file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9ae2afa-84ba-4fb9-85fb-c5ffc4a572b5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e9ae2afa-84ba-4fb9-85fb-c5ffc4a572b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJMVxUZhyqye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNDKeTY3yrf7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "ac5a4c23-b2c5-46b1-e48d-4cfa73a0c3d4"
      },
      "source": [
        "# download the dataset and unzip\n",
        "!kaggle competitions download sentiment-analysis-on-movie-reviews\n",
        "!unzip train.tsv.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.tsv.zip to /content\n",
            "  0% 0.00/494k [00:00<?, ?B/s]\n",
            "100% 494k/494k [00:00<00:00, 75.7MB/s]\n",
            "Downloading sampleSubmission.csv to /content\n",
            "  0% 0.00/583k [00:00<?, ?B/s]\n",
            "100% 583k/583k [00:00<00:00, 81.7MB/s]\n",
            "Downloading train.tsv.zip to /content\n",
            "  0% 0.00/1.28M [00:00<?, ?B/s]\n",
            "100% 1.28M/1.28M [00:00<00:00, 87.5MB/s]\n",
            "Archive:  train.tsv.zip\n",
            "  inflating: train.tsv               \n",
            "Archive:  test.tsv.zip\n",
            "  inflating: test.tsv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9KBpeXiXCvR",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing \n",
        "\n",
        "The model cannot accept strings as input, thus in this section we convert each element of the dataset into an integer-encoded vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4Q4x31XFTa",
        "colab_type": "text"
      },
      "source": [
        "###  Text processing\n",
        "\n",
        "Descriptions:\n",
        "- `tokenizer`: function that takes as input a string of text and returns a list of tokens \n",
        "- `Vocabularly`: class that stores the vocab found in the dataset and creates a mapping from token to integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnIJ2nYKyt8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text): \n",
        "    text = text.lower()\n",
        "    text = re.sub(\"-rrb-\",\"\", text)\n",
        "    text = re.sub(\"-lrb-\",\"\", text)\n",
        "    tokens = spacy_en.tokenizer(text)\n",
        "    #tokens = [tok for tok in tokens if tok.is_stop == False]\n",
        "    tokens = [tok.lemma_ for tok in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd_S8mQ3yv0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary:\n",
        "      \n",
        "    def __init__(self, vocabCount, min_freq):\n",
        "        \n",
        "        # UNK tokens \n",
        "        self.PAD_token = 0\n",
        "        self.UNK_token = 1\n",
        "        self.vocabCount = vocabCount\n",
        "        self.min_freq = min_freq\n",
        "        # initialize list of words and vocab dictionary\n",
        "        self.wordlist = [\"<pad>\", \"<unk>\"]\n",
        "        self.word2index = {}\n",
        "        # build vocab\n",
        "        self.build_vocab(vocabCount)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2index)\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        return self.word2index.get(word, 1)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(vocab.word2index)\n",
        "\n",
        "    def build_vocab(self, vocabCount):\n",
        "        # sort vocab s.t. words that occur most frequently added first\n",
        "        svocabCount = {k: v for k, v in reversed(sorted(vocabCount.items(), \n",
        "                                                      key=lambda item: item[1]))}\n",
        "        \n",
        "        for word in svocabCount:\n",
        "            if svocabCount[word] >= self.min_freq:\n",
        "                self.wordlist.append(word)\n",
        "        self.word2index.update({tok: i for i, tok in enumerate(self.wordlist)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BjzUyjQXSkH",
        "colab_type": "text"
      },
      "source": [
        "### Loading and processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fvsgHZ4y39-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('/content/train.tsv', sep=\"\\t\", \n",
        "                         encoding=\"utf_8_sig\")\n",
        "\n",
        "phrases = onp.array(train_data.iloc[:, 2])\n",
        "target = onp.array(train_data.iloc[:, 3])\n",
        "\n",
        "# create train and validation sets\n",
        "X_train , X_val, y_train , y_val = train_test_split(phrases, target, \n",
        "                                                    test_size = 0.2, random_state=42)\n",
        "\n",
        "# create validation and test sets\n",
        "X_val , X_test, y_val , y_test = train_test_split(X_val, y_val, \n",
        "                                                    test_size = 0.4, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgIo9eB0b6em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = [tokenizer(phrase) for phrase in X_train]\n",
        "X_val = [tokenizer(phrase) for phrase in X_val]\n",
        "X_test = [tokenizer(phrase) for phrase in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ5Oc8YQy5LU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "0d102873-41a8-4fb6-fff2-9b9999fdb7b6"
      },
      "source": [
        "print(\"Length of train dataset: {} \\nLength of validation dataset: {} \\nLength of test dataset: {}\".format(len(X_train), len(X_val), len(X_test)))"
      ],
      "execution_count": 449,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train dataset: 124848 \n",
            "Length of validation dataset: 18727 \n",
            "Length of test dataset: 12485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX2vh4hNXXJt",
        "colab_type": "text"
      },
      "source": [
        "### Creating vocabularly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qadQIG_5zL_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabCount = Counter([item for sublist in X_train for item in sublist])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTuH0nu_zNNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary(vocabCount, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kl8VvbqXZ5e",
        "colab_type": "text"
      },
      "source": [
        "### Converting tokens to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwHQwt_FzOM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_trainNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_train]\n",
        "X_valNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_val]\n",
        "X_testNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R76fAavVlrWS",
        "colab_type": "text"
      },
      "source": [
        "Our pre-processing might have resulted in some now empty lists. These can cause problems later so we just fill them with padding (another possibility would be to remove these examples as they contain no useful information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgVKsVWLhGyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_trainNum):\n",
        "    if len(el) == 0:\n",
        "        X_trainNum[i] = onp.array([0])\n",
        "\n",
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_valNum):\n",
        "    if len(el) == 0:\n",
        "        X_valNum[i] = onp.array([0])\n",
        "\n",
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_testNum):\n",
        "    if len(el) == 0:\n",
        "        X_testNum[i] = onp.array([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu8rh_rzXeX4",
        "colab_type": "text"
      },
      "source": [
        "##  Bag of vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjJkOvAS40hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):  \n",
        "        X = self.X[idx]\n",
        "        Y = self.y[idx]\n",
        "        return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEHvGccmpVzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(sequences):\n",
        "  # adapted and simplified from https://github.com/keras-team/keras-preprocessing/blob\n",
        "  # /master/keras_preprocessing/sequence.py\n",
        "\n",
        "  num_samples = len(sequences)\n",
        "  lengths = []\n",
        "\n",
        "  for sequence in sequences:\n",
        "    lengths.append(len(sequence))\n",
        "  \n",
        "  max_len = onp.max(lengths)\n",
        "\n",
        "  x = onp.full((num_samples, max_len), 0)\n",
        "\n",
        "  for idx, sequence in enumerate(sequences):\n",
        "    x[idx, :len(sequence)] = sequence\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anb_5qwMzUSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numpy_collate(batch):\n",
        "  data = [item[0] for item in batch]\n",
        "  targets = onp.array([item[1] for item in batch])\n",
        "\n",
        "\n",
        "  data = pad_sequences(data)\n",
        "\n",
        "  return jnp.asarray(data), jnp.array(targets)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUWzazSX_IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingset = WordDataset(X_trainNum, y_train)\n",
        "valset = WordDataset(X_valNum, y_val)\n",
        "testset = WordDataset(X_testNum, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMDqdmYzYPlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_generator = NumpyLoader(trainingset, batch_size=64)\n",
        "val_generator = NumpyLoader(valset, batch_size=len(y_val))\n",
        "test_generator = NumpyLoader(testset, batch_size=len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK8oH-dNYQem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_eval = NumpyLoader(trainingset, batch_size=len(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkpidCCTYZk7",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77KCW_iNYiT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "key, subkey = jax.random.split(key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqQS_9dqr5bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _forward(inputs, rng, is_training=True):\n",
        "\n",
        "  # create mask to adjust for padding\n",
        "  mask = jnp.where(inputs==0,0,1)\n",
        "\n",
        "  # define embedding layer\n",
        "  embed = hk.Embed(len(vocab), embed_dim=300)\n",
        "\n",
        "  # pass through network\n",
        "  x = embed(inputs)\n",
        "  x = sum_pooling(x, mask)\n",
        "  x = hk.Linear(128)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(64)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(5)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7of8bc2ZuMJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def sum_pooling(inputs, mask):\n",
        "\n",
        "  return jnp.einsum(\"ijk, ij -> ik\", inputs, mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMCQy4_HuX09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(rng, rate, x, is_training):\n",
        "\n",
        "  if is_training:\n",
        "    return hk.dropout(rng, rate, x)\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5yoGtZvvW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert into pure functions\n",
        "net = hk.transform(_forward)\n",
        "\n",
        "# initialise parameters (need sample inputs for this)\n",
        "sample_input, _ = next(iter(training_generator))\n",
        "params = net.init(jax.random.PRNGKey(42), sample_input, subkey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfz0fzeY7Zu",
        "colab_type": "text"
      },
      "source": [
        "### Loss and evalulation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGNiMADwvkSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def loss(params, batch, rng, is_training=True):\n",
        "  inputs, targets = batch\n",
        "  preds = net.apply(params, inputs, rng, is_training)\n",
        "  targets = hk.one_hot(targets, 5)\n",
        "  return -np.mean(np.sum(jax.nn.log_softmax(preds) * targets, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGNLSOKtbjOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def eval_batch(params, batch, rng):\n",
        "  inputs, targets = batch\n",
        "  logits = net.apply(params, inputs, rng, False)\n",
        "\n",
        "  # for accuracy\n",
        "  predicted_label = jnp.argmax(logits, axis=-1)\n",
        "  correct = np.sum(jnp.equal(predicted_label, targets))\n",
        "\n",
        "  # for loss\n",
        "  one_hot_targets = hk.one_hot(targets, 5)\n",
        "  loss = -np.sum((np.sum(jax.nn.log_softmax(logits) * one_hot_targets, axis=1)))\n",
        "\n",
        "  return correct.astype(jnp.float32), loss.astype(jnp.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IficpzPcbCi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(params, dataLoader, rng):\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  total = 0\n",
        "  for batch in dataLoader:\n",
        "    c, l = eval_batch(params, batch, rng)\n",
        "    correct += c\n",
        "    loss += l\n",
        "    total += batch[1].shape[0]\n",
        "  acc = correct/total\n",
        "  loss = loss/total\n",
        "  return {\"acc\": acc, \"loss\": loss}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeCiRdH9cToh",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5YtUxruLsPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(step):\n",
        "  \n",
        "  steps_per_epoch = jnp.ceil(len(training_generator.dataset)/ 64)\n",
        "  current_epoch = step / steps_per_epoch \n",
        "  factor = current_epoch // 5\n",
        " \n",
        "  return 0.85**factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AR0bkAM0EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer():\n",
        "  \n",
        "  return optix.chain(optix.adam(1e-4),\n",
        "                     optix.scale_by_schedule(lr_schedule))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT1TReLn3HOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " opt_state = make_optimizer().init(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10cK7vVlybJ",
        "colab_type": "text"
      },
      "source": [
        "### Update function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d62Ksc5r153l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def update(params, opt_state, batch, rng):\n",
        "  grads = jax.grad(loss)(params, batch, rng)\n",
        "  updates, opt_state = make_optimizer().update(grads, opt_state)\n",
        "  new_params = optix.apply_updates(params, updates)\n",
        "  return new_params, opt_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09z8rk0cp9G",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jzK6lm5mcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "e853eb9c-6a99-4fc2-fb02-9d69b32c0b96"
      },
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "for step in range(50):\n",
        "  \n",
        "  for b_idx, batch in enumerate(training_generator):\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state = update(params, opt_state, batch, subkey)\n",
        "\n",
        "  # dont need a new random key for evalulation\n",
        "  train_metrics = evaluate(params, training_eval, subkey)\n",
        "  train_accuracy, train_loss = train_metrics[\"acc\"], train_metrics[\"loss\"]\n",
        "  train_accuracy, train_loss = jax.device_get((train_accuracy,\n",
        "                                               train_loss))\n",
        "  \n",
        "  val_metrics = evaluate(params, val_generator, subkey)\n",
        "  val_accuracy, val_loss = val_metrics[\"acc\"], val_metrics[\"loss\"]\n",
        "  val_accuracy, val_loss = jax.device_get((val_accuracy,\n",
        "                                               val_loss))\n",
        "  \n",
        "  print((\"Epoch: {} \\t Loss (train): {:.3f} (val): {:.3f} \\t\" +\n",
        "              \"Acc (train) {:.3f} (val): {:.3f}\").format(step + 1,\n",
        "                            train_loss, val_loss, train_accuracy, \n",
        "                            val_accuracy))"
      ],
      "execution_count": 472,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Loss (train): 1.277 (val): 1.287 \tAcc (train) 0.514 (val): 0.505\n",
            "Epoch: 2 \t Loss (train): 1.235 (val): 1.248 \tAcc (train) 0.514 (val): 0.505\n",
            "Epoch: 3 \t Loss (train): 1.202 (val): 1.220 \tAcc (train) 0.516 (val): 0.508\n",
            "Epoch: 4 \t Loss (train): 1.170 (val): 1.192 \tAcc (train) 0.523 (val): 0.516\n",
            "Epoch: 5 \t Loss (train): 1.130 (val): 1.157 \tAcc (train) 0.536 (val): 0.528\n",
            "Epoch: 6 \t Loss (train): 1.095 (val): 1.127 \tAcc (train) 0.549 (val): 0.541\n",
            "Epoch: 7 \t Loss (train): 1.060 (val): 1.098 \tAcc (train) 0.561 (val): 0.550\n",
            "Epoch: 8 \t Loss (train): 1.025 (val): 1.069 \tAcc (train) 0.574 (val): 0.561\n",
            "Epoch: 9 \t Loss (train): 0.995 (val): 1.045 \tAcc (train) 0.584 (val): 0.570\n",
            "Epoch: 10 \t Loss (train): 0.966 (val): 1.022 \tAcc (train) 0.596 (val): 0.579\n",
            "Epoch: 11 \t Loss (train): 0.942 (val): 1.004 \tAcc (train) 0.606 (val): 0.588\n",
            "Epoch: 12 \t Loss (train): 0.922 (val): 0.989 \tAcc (train) 0.613 (val): 0.592\n",
            "Epoch: 13 \t Loss (train): 0.904 (val): 0.976 \tAcc (train) 0.620 (val): 0.596\n",
            "Epoch: 14 \t Loss (train): 0.886 (val): 0.964 \tAcc (train) 0.627 (val): 0.602\n",
            "Epoch: 15 \t Loss (train): 0.870 (val): 0.954 \tAcc (train) 0.632 (val): 0.606\n",
            "Epoch: 16 \t Loss (train): 0.857 (val): 0.945 \tAcc (train) 0.637 (val): 0.609\n",
            "Epoch: 17 \t Loss (train): 0.844 (val): 0.937 \tAcc (train) 0.643 (val): 0.614\n",
            "Epoch: 18 \t Loss (train): 0.833 (val): 0.931 \tAcc (train) 0.647 (val): 0.616\n",
            "Epoch: 19 \t Loss (train): 0.822 (val): 0.925 \tAcc (train) 0.653 (val): 0.619\n",
            "Epoch: 20 \t Loss (train): 0.812 (val): 0.919 \tAcc (train) 0.656 (val): 0.622\n",
            "Epoch: 21 \t Loss (train): 0.804 (val): 0.915 \tAcc (train) 0.659 (val): 0.624\n",
            "Epoch: 22 \t Loss (train): 0.796 (val): 0.911 \tAcc (train) 0.663 (val): 0.624\n",
            "Epoch: 23 \t Loss (train): 0.788 (val): 0.907 \tAcc (train) 0.666 (val): 0.627\n",
            "Epoch: 24 \t Loss (train): 0.781 (val): 0.903 \tAcc (train) 0.668 (val): 0.629\n",
            "Epoch: 25 \t Loss (train): 0.774 (val): 0.900 \tAcc (train) 0.672 (val): 0.630\n",
            "Epoch: 26 \t Loss (train): 0.768 (val): 0.897 \tAcc (train) 0.675 (val): 0.631\n",
            "Epoch: 27 \t Loss (train): 0.762 (val): 0.895 \tAcc (train) 0.677 (val): 0.633\n",
            "Epoch: 28 \t Loss (train): 0.757 (val): 0.893 \tAcc (train) 0.679 (val): 0.635\n",
            "Epoch: 29 \t Loss (train): 0.751 (val): 0.890 \tAcc (train) 0.681 (val): 0.636\n",
            "Epoch: 30 \t Loss (train): 0.746 (val): 0.887 \tAcc (train) 0.685 (val): 0.638\n",
            "Epoch: 31 \t Loss (train): 0.741 (val): 0.886 \tAcc (train) 0.686 (val): 0.638\n",
            "Epoch: 32 \t Loss (train): 0.737 (val): 0.884 \tAcc (train) 0.689 (val): 0.639\n",
            "Epoch: 33 \t Loss (train): 0.733 (val): 0.883 \tAcc (train) 0.689 (val): 0.641\n",
            "Epoch: 34 \t Loss (train): 0.729 (val): 0.881 \tAcc (train) 0.691 (val): 0.641\n",
            "Epoch: 35 \t Loss (train): 0.726 (val): 0.881 \tAcc (train) 0.691 (val): 0.641\n",
            "Epoch: 36 \t Loss (train): 0.722 (val): 0.880 \tAcc (train) 0.694 (val): 0.642\n",
            "Epoch: 37 \t Loss (train): 0.718 (val): 0.879 \tAcc (train) 0.695 (val): 0.644\n",
            "Epoch: 38 \t Loss (train): 0.715 (val): 0.878 \tAcc (train) 0.697 (val): 0.645\n",
            "Epoch: 39 \t Loss (train): 0.712 (val): 0.878 \tAcc (train) 0.698 (val): 0.645\n",
            "Epoch: 40 \t Loss (train): 0.709 (val): 0.876 \tAcc (train) 0.700 (val): 0.646\n",
            "Epoch: 41 \t Loss (train): 0.706 (val): 0.875 \tAcc (train) 0.701 (val): 0.647\n",
            "Epoch: 42 \t Loss (train): 0.703 (val): 0.874 \tAcc (train) 0.702 (val): 0.648\n",
            "Epoch: 43 \t Loss (train): 0.701 (val): 0.874 \tAcc (train) 0.703 (val): 0.649\n",
            "Epoch: 44 \t Loss (train): 0.699 (val): 0.874 \tAcc (train) 0.703 (val): 0.648\n",
            "Epoch: 45 \t Loss (train): 0.696 (val): 0.873 \tAcc (train) 0.705 (val): 0.648\n",
            "Epoch: 46 \t Loss (train): 0.694 (val): 0.872 \tAcc (train) 0.706 (val): 0.649\n",
            "Epoch: 47 \t Loss (train): 0.692 (val): 0.872 \tAcc (train) 0.707 (val): 0.649\n",
            "Epoch: 48 \t Loss (train): 0.690 (val): 0.872 \tAcc (train) 0.708 (val): 0.649\n",
            "Epoch: 49 \t Loss (train): 0.689 (val): 0.872 \tAcc (train) 0.708 (val): 0.649\n",
            "Epoch: 50 \t Loss (train): 0.686 (val): 0.872 \tAcc (train) 0.709 (val): 0.648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BuBBhXl9q6",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL7fH9Fbd8Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metrics = evaluate(params, training_eval, subkey)\n",
        "val_metrics = evaluate(params, val_generator, subkey)\n",
        "test_metrics = evaluate(params, test_generator, subkey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dytbd3BdlKNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "61d9b374-fbe4-49a1-827e-52f565cc5c13"
      },
      "source": [
        "print(\"Train metrics: \\n{}\".format(train_metrics))\n",
        "print(\"Val metrics: \\n{}\".format(val_metrics))\n",
        "print(\"Test metrics: \\n{}\".format(test_metrics))"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metrics: \n",
            "{'acc': DeviceArray(0.7088059, dtype=float32), 'loss': DeviceArray(0.68640625, dtype=float32)}\n",
            "Val metrics: \n",
            "{'acc': DeviceArray(0.6484221, dtype=float32), 'loss': DeviceArray(0.8717084, dtype=float32)}\n",
            "Test metrics: \n",
            "{'acc': DeviceArray(0.64989984, dtype=float32), 'loss': DeviceArray(0.86235493, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
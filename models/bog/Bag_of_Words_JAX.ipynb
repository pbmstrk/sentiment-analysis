{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Bag_of_Words_JAX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO94xALRoix3ztsxKjdnsUM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m-fAnSxWILE",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis: Bag-of-Words (JAX and Haiku)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEa-uGfWugY",
        "colab_type": "text"
      },
      "source": [
        "The notebook is roughly divided into two sections: pre-processing and the actual model.\n",
        "\n",
        "## Setup and downloading data\n",
        "\n",
        "We use the Kaggle API to download the data. \n",
        "\n",
        "**N.B.** If you want to run the notebook you need to upload a `kaggle.json` file which contains your API credentials. Instructions for downloading the file from Kaggle can be found [here](https://github.com/Kaggle/kaggle-api).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fit3D94Gx29r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the kaggle API and haiku\n",
        "!pip install kaggle -q\n",
        "!pip install dm-haiku -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG9aRGVOx8dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data wrangling\n",
        "import numpy as onp\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# language model \n",
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "# jax and haiku\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from jax.experimental import optix\n",
        "\n",
        "# data pipeline\n",
        "from torch.utils import data\n",
        "\n",
        "from collections import Counter\n",
        "import copy\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4UIHccFygj8",
        "colab_type": "code",
        "outputId": "ea8a7c97-02c5-4c3b-cebf-9a7e7fc15b29",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "# upload kaggle.json file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b9d81a0c-3b2a-4800-86b0-fdcc9d02d149\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b9d81a0c-3b2a-4800-86b0-fdcc9d02d149\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJMVxUZhyqye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNDKeTY3yrf7",
        "colab_type": "code",
        "outputId": "cb319446-e350-420d-b92e-5dd4cfcd5b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# download the dataset and unzip\n",
        "!kaggle competitions download sentiment-analysis-on-movie-reviews\n",
        "!unzip train.tsv.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "train.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  train.tsv.zip\n",
            "replace train.tsv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9KBpeXiXCvR",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing \n",
        "\n",
        "The model cannot accept strings as input, thus in this section we convert each element of the dataset into an integer-encoded vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4Q4x31XFTa",
        "colab_type": "text"
      },
      "source": [
        "###  Text processing\n",
        "\n",
        "Descriptions:\n",
        "- `tokenizer`: function that takes as input a string of text and returns a list of tokens \n",
        "- `Vocabularly`: class that stores the vocab found in the dataset and creates a mapping from token to integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnIJ2nYKyt8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text): \n",
        "    text = text.lower()\n",
        "    text = re.sub(\"-rrb-\",\"\", text)\n",
        "    text = re.sub(\"-lrb-\",\"\", text)\n",
        "    tokens = spacy_en.tokenizer(text)\n",
        "    #tokens = [tok for tok in tokens if tok.is_stop == False]\n",
        "    tokens = [tok.lemma_ for tok in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd_S8mQ3yv0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary:\n",
        "      \n",
        "    def __init__(self, vocabCount, min_freq):\n",
        "        \n",
        "        # UNK tokens \n",
        "        self.PAD_token = 0\n",
        "        self.UNK_token = 1\n",
        "        self.vocabCount = vocabCount\n",
        "        self.min_freq = min_freq\n",
        "        # initialize list of words and vocab dictionary\n",
        "        self.wordlist = [\"<pad>\", \"<unk>\"]\n",
        "        self.word2index = {}\n",
        "        # build vocab\n",
        "        self.build_vocab(vocabCount)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2index)\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        return self.word2index.get(word, 1)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(vocab.word2index)\n",
        "\n",
        "    def build_vocab(self, vocabCount):\n",
        "        # sort vocab s.t. words that occur most frequently added first\n",
        "        svocabCount = {k: v for k, v in reversed(sorted(vocabCount.items(), \n",
        "                                                      key=lambda item: item[1]))}\n",
        "        \n",
        "        for word in svocabCount:\n",
        "            if svocabCount[word] >= self.min_freq:\n",
        "                self.wordlist.append(word)\n",
        "        self.word2index.update({tok: i for i, tok in enumerate(self.wordlist)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BjzUyjQXSkH",
        "colab_type": "text"
      },
      "source": [
        "### Loading and processing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fvsgHZ4y39-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('/content/train.tsv', sep=\"\\t\", \n",
        "                         encoding=\"utf_8_sig\")\n",
        "\n",
        "phrases = onp.array(train_data.iloc[:, 2])\n",
        "target = onp.array(train_data.iloc[:, 3])\n",
        "\n",
        "# create train and validation sets\n",
        "X_train , X_val, y_train , y_val = train_test_split(phrases, target, \n",
        "                                                    test_size = 0.2, random_state=42)\n",
        "\n",
        "# create validation and test sets\n",
        "X_val , X_test, y_val , y_test = train_test_split(X_val, y_val, \n",
        "                                                    test_size = 0.4, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgIo9eB0b6em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = [tokenizer(phrase) for phrase in X_train]\n",
        "X_val = [tokenizer(phrase) for phrase in X_val]\n",
        "X_test = [tokenizer(phrase) for phrase in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ5Oc8YQy5LU",
        "colab_type": "code",
        "outputId": "fbb36a4d-c01d-4072-d5d0-d26d74b8d11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print(\"Length of train dataset: {} \\nLength of validation dataset: {} \\nLength of test dataset: {}\".format(len(X_train), len(X_val), len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train dataset: 124848 \n",
            "Length of validation dataset: 18727 \n",
            "Length of test dataset: 12485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX2vh4hNXXJt",
        "colab_type": "text"
      },
      "source": [
        "### Creating vocabularly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qadQIG_5zL_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabCount = Counter([item for sublist in X_train for item in sublist])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTuH0nu_zNNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary(vocabCount, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kl8VvbqXZ5e",
        "colab_type": "text"
      },
      "source": [
        "### Converting tokens to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwHQwt_FzOM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_trainNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_train]\n",
        "X_valNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_val]\n",
        "X_testNum = [onp.array([vocab[word] for word in phrase]) for phrase in X_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R76fAavVlrWS",
        "colab_type": "text"
      },
      "source": [
        "Our pre-processing might have resulted in some now empty lists. These can cause problems later so we just fill them with padding (another possibility would be to remove these examples as they contain no useful information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgVKsVWLhGyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_trainNum):\n",
        "    if len(el) == 0:\n",
        "        X_trainNum[i] = onp.array([0])\n",
        "\n",
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_valNum):\n",
        "    if len(el) == 0:\n",
        "        X_valNum[i] = onp.array([0])\n",
        "\n",
        "# make sure each tensor actually has values\n",
        "for i, el in enumerate(X_testNum):\n",
        "    if len(el) == 0:\n",
        "        X_testNum[i] = onp.array([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu8rh_rzXeX4",
        "colab_type": "text"
      },
      "source": [
        "##  Bag of vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjJkOvAS40hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):  \n",
        "        X = self.X[idx]\n",
        "        Y = self.y[idx]\n",
        "        return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEHvGccmpVzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(sequences):\n",
        "  # adapted and simplified from https://github.com/keras-team/keras-preprocessing/blob\n",
        "  # /master/keras_preprocessing/sequence.py\n",
        "\n",
        "  num_samples = len(sequences)\n",
        "  lengths = []\n",
        "\n",
        "  for sequence in sequences:\n",
        "    lengths.append(len(sequence))\n",
        "  \n",
        "  max_len = onp.max(lengths)\n",
        "\n",
        "  x = onp.full((num_samples, max_len), 0)\n",
        "\n",
        "  for idx, sequence in enumerate(sequences):\n",
        "    x[idx, :len(sequence)] = sequence\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anb_5qwMzUSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numpy_collate(batch):\n",
        "  data = [item[0] for item in batch]\n",
        "  targets = onp.array([item[1] for item in batch])\n",
        "\n",
        "\n",
        "  data = pad_sequences(data)\n",
        "\n",
        "  return jnp.asarray(data), jnp.array(targets)\n",
        "\n",
        "class NumpyLoader(data.DataLoader):\n",
        "  def __init__(self, dataset, batch_size=1,\n",
        "                shuffle=False, sampler=None,\n",
        "                batch_sampler=None, num_workers=0,\n",
        "                pin_memory=False, drop_last=False,\n",
        "                timeout=0, worker_init_fn=None):\n",
        "    super(self.__class__, self).__init__(dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        sampler=sampler,\n",
        "        batch_sampler=batch_sampler,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=numpy_collate,\n",
        "        pin_memory=pin_memory,\n",
        "        drop_last=drop_last,\n",
        "        timeout=timeout,\n",
        "        worker_init_fn=worker_init_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReUWzazSX_IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingset = WordDataset(X_trainNum, y_train)\n",
        "valset = WordDataset(X_valNum, y_val)\n",
        "testset = WordDataset(X_testNum, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMDqdmYzYPlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_generator = NumpyLoader(trainingset, batch_size=64)\n",
        "val_generator = NumpyLoader(valset, batch_size=len(y_val))\n",
        "test_generator = NumpyLoader(testset, batch_size=len(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK8oH-dNYQem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_eval = NumpyLoader(trainingset, batch_size=len(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkpidCCTYZk7",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77KCW_iNYiT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "key, subkey = jax.random.split(key)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqQS_9dqr5bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _forward(inputs, rng, is_training=True):\n",
        "\n",
        "  # create mask to adjust for padding\n",
        "  mask = jnp.where(inputs==0,0,1)\n",
        "\n",
        "  # define embedding layer\n",
        "  embed = hk.Embed(len(vocab), embed_dim=300)\n",
        "\n",
        "  # pass through network\n",
        "  x = embed(inputs)\n",
        "  x = sum_pooling(x, mask)\n",
        "  x = hk.Linear(128)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(64)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(5)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7of8bc2ZuMJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def sum_pooling(inputs, mask):\n",
        "\n",
        "  return jnp.einsum(\"ijk, ij -> ik\", inputs, mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMCQy4_HuX09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(rng, rate, x, is_training):\n",
        "\n",
        "  if is_training:\n",
        "    return hk.dropout(rng, rate, x)\n",
        "  else:\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5yoGtZvvW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert into pure functions\n",
        "net = hk.transform(_forward)\n",
        "\n",
        "# initialise parameters (need sample inputs for this)\n",
        "sample_input, _ = next(iter(training_generator))\n",
        "params = net.init(jax.random.PRNGKey(42), sample_input, subkey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfz0fzeY7Zu",
        "colab_type": "text"
      },
      "source": [
        "### Loss and evalulation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGNiMADwvkSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def loss(params, batch, rng, is_training=True):\n",
        "  inputs, targets = batch\n",
        "  preds = net.apply(params, inputs, rng, is_training)\n",
        "  targets = hk.one_hot(targets, 5)\n",
        "  return -jnp.mean(jnp.sum(jax.nn.log_softmax(preds) * targets, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGNLSOKtbjOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def eval_batch(params, batch, rng):\n",
        "  inputs, targets = batch\n",
        "  logits = net.apply(params, inputs, rng, False)\n",
        "\n",
        "  # for accuracy\n",
        "  predicted_label = jnp.argmax(logits, axis=-1)\n",
        "  correct = jnp.sum(jnp.equal(predicted_label, targets))\n",
        "\n",
        "  # for loss\n",
        "  one_hot_targets = hk.one_hot(targets, 5)\n",
        "  loss = -jnp.sum((jnp.sum(jax.nn.log_softmax(logits) * one_hot_targets, axis=1)))\n",
        "\n",
        "  return correct.astype(jnp.float32), loss.astype(jnp.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IficpzPcbCi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(params, dataLoader, rng):\n",
        "  correct = 0\n",
        "  loss = 0\n",
        "  total = 0\n",
        "  for batch in dataLoader:\n",
        "    c, l = eval_batch(params, batch, rng)\n",
        "    correct += c\n",
        "    loss += l\n",
        "    total += batch[1].shape[0]\n",
        "  acc = correct/total\n",
        "  loss = loss/total\n",
        "  return {\"acc\": acc, \"loss\": loss}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeCiRdH9cToh",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5YtUxruLsPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(step):\n",
        "  \n",
        "  steps_per_epoch = jnp.ceil(len(training_generator.dataset)/ 64)\n",
        "  current_epoch = step / steps_per_epoch \n",
        "  factor = current_epoch // 5\n",
        " \n",
        "  return 0.85**factor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5AR0bkAM0EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer():\n",
        "  \n",
        "  return optix.chain(optix.adam(1e-4),\n",
        "                     optix.scale_by_schedule(lr_schedule))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT1TReLn3HOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " opt_state = make_optimizer().init(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z10cK7vVlybJ",
        "colab_type": "text"
      },
      "source": [
        "### Update function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d62Ksc5r153l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def update(params, opt_state, batch, rng):\n",
        "  grads = jax.grad(loss)(params, batch, rng)\n",
        "  updates, opt_state = make_optimizer().update(grads, opt_state)\n",
        "  new_params = optix.apply_updates(params, updates)\n",
        "  return new_params, opt_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09z8rk0cp9G",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jzK6lm5mcq",
        "colab_type": "code",
        "outputId": "d9982eb0-2db2-42e9-8066-c5ea5df7c576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        }
      },
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "for step in range(50):\n",
        "  \n",
        "  for b_idx, batch in enumerate(training_generator):\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state = update(params, opt_state, batch, subkey)\n",
        "\n",
        "  # dont need a new random key for evalulation\n",
        "  train_metrics = evaluate(params, training_eval, subkey)\n",
        "  train_accuracy, train_loss = train_metrics[\"acc\"], train_metrics[\"loss\"]\n",
        "  train_accuracy, train_loss = jax.device_get((train_accuracy,\n",
        "                                               train_loss))\n",
        "  \n",
        "  val_metrics = evaluate(params, val_generator, subkey)\n",
        "  val_accuracy, val_loss = val_metrics[\"acc\"], val_metrics[\"loss\"]\n",
        "  val_accuracy, val_loss = jax.device_get((val_accuracy,\n",
        "                                               val_loss))\n",
        "  \n",
        "  print((\"Epoch: {} \\t Loss (train): {:.3f} (val): {:.3f} \\t\" +\n",
        "              \"Acc (train) {:.3f} (val): {:.3f}\").format(step + 1,\n",
        "                            train_loss, val_loss, train_accuracy, \n",
        "                            val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Loss (train): 1.277 (val): 1.287 \tAcc (train) 0.514 (val): 0.505\n",
            "Epoch: 2 \t Loss (train): 1.234 (val): 1.248 \tAcc (train) 0.514 (val): 0.505\n",
            "Epoch: 3 \t Loss (train): 1.202 (val): 1.220 \tAcc (train) 0.516 (val): 0.508\n",
            "Epoch: 4 \t Loss (train): 1.170 (val): 1.192 \tAcc (train) 0.523 (val): 0.516\n",
            "Epoch: 5 \t Loss (train): 1.131 (val): 1.158 \tAcc (train) 0.536 (val): 0.528\n",
            "Epoch: 6 \t Loss (train): 1.095 (val): 1.127 \tAcc (train) 0.549 (val): 0.540\n",
            "Epoch: 7 \t Loss (train): 1.060 (val): 1.098 \tAcc (train) 0.561 (val): 0.549\n",
            "Epoch: 8 \t Loss (train): 1.025 (val): 1.069 \tAcc (train) 0.574 (val): 0.560\n",
            "Epoch: 9 \t Loss (train): 0.994 (val): 1.045 \tAcc (train) 0.585 (val): 0.570\n",
            "Epoch: 10 \t Loss (train): 0.966 (val): 1.022 \tAcc (train) 0.595 (val): 0.579\n",
            "Epoch: 11 \t Loss (train): 0.942 (val): 1.004 \tAcc (train) 0.606 (val): 0.587\n",
            "Epoch: 12 \t Loss (train): 0.922 (val): 0.989 \tAcc (train) 0.613 (val): 0.592\n",
            "Epoch: 13 \t Loss (train): 0.904 (val): 0.975 \tAcc (train) 0.620 (val): 0.596\n",
            "Epoch: 14 \t Loss (train): 0.886 (val): 0.964 \tAcc (train) 0.627 (val): 0.602\n",
            "Epoch: 15 \t Loss (train): 0.870 (val): 0.954 \tAcc (train) 0.632 (val): 0.606\n",
            "Epoch: 16 \t Loss (train): 0.857 (val): 0.946 \tAcc (train) 0.636 (val): 0.610\n",
            "Epoch: 17 \t Loss (train): 0.844 (val): 0.938 \tAcc (train) 0.643 (val): 0.614\n",
            "Epoch: 18 \t Loss (train): 0.833 (val): 0.931 \tAcc (train) 0.647 (val): 0.617\n",
            "Epoch: 19 \t Loss (train): 0.822 (val): 0.925 \tAcc (train) 0.653 (val): 0.619\n",
            "Epoch: 20 \t Loss (train): 0.812 (val): 0.919 \tAcc (train) 0.656 (val): 0.621\n",
            "Epoch: 21 \t Loss (train): 0.804 (val): 0.915 \tAcc (train) 0.659 (val): 0.623\n",
            "Epoch: 22 \t Loss (train): 0.796 (val): 0.911 \tAcc (train) 0.662 (val): 0.625\n",
            "Epoch: 23 \t Loss (train): 0.788 (val): 0.907 \tAcc (train) 0.666 (val): 0.628\n",
            "Epoch: 24 \t Loss (train): 0.781 (val): 0.904 \tAcc (train) 0.669 (val): 0.630\n",
            "Epoch: 25 \t Loss (train): 0.774 (val): 0.900 \tAcc (train) 0.672 (val): 0.631\n",
            "Epoch: 26 \t Loss (train): 0.768 (val): 0.897 \tAcc (train) 0.675 (val): 0.632\n",
            "Epoch: 27 \t Loss (train): 0.762 (val): 0.895 \tAcc (train) 0.677 (val): 0.632\n",
            "Epoch: 28 \t Loss (train): 0.757 (val): 0.893 \tAcc (train) 0.679 (val): 0.633\n",
            "Epoch: 29 \t Loss (train): 0.752 (val): 0.890 \tAcc (train) 0.682 (val): 0.635\n",
            "Epoch: 30 \t Loss (train): 0.746 (val): 0.887 \tAcc (train) 0.685 (val): 0.638\n",
            "Epoch: 31 \t Loss (train): 0.741 (val): 0.886 \tAcc (train) 0.686 (val): 0.638\n",
            "Epoch: 32 \t Loss (train): 0.737 (val): 0.884 \tAcc (train) 0.688 (val): 0.639\n",
            "Epoch: 33 \t Loss (train): 0.733 (val): 0.883 \tAcc (train) 0.689 (val): 0.640\n",
            "Epoch: 34 \t Loss (train): 0.729 (val): 0.882 \tAcc (train) 0.691 (val): 0.639\n",
            "Epoch: 35 \t Loss (train): 0.726 (val): 0.881 \tAcc (train) 0.691 (val): 0.641\n",
            "Epoch: 36 \t Loss (train): 0.722 (val): 0.880 \tAcc (train) 0.694 (val): 0.642\n",
            "Epoch: 37 \t Loss (train): 0.718 (val): 0.880 \tAcc (train) 0.695 (val): 0.644\n",
            "Epoch: 38 \t Loss (train): 0.715 (val): 0.879 \tAcc (train) 0.697 (val): 0.644\n",
            "Epoch: 39 \t Loss (train): 0.712 (val): 0.878 \tAcc (train) 0.697 (val): 0.644\n",
            "Epoch: 40 \t Loss (train): 0.709 (val): 0.876 \tAcc (train) 0.699 (val): 0.645\n",
            "Epoch: 41 \t Loss (train): 0.706 (val): 0.875 \tAcc (train) 0.701 (val): 0.647\n",
            "Epoch: 42 \t Loss (train): 0.703 (val): 0.875 \tAcc (train) 0.702 (val): 0.648\n",
            "Epoch: 43 \t Loss (train): 0.701 (val): 0.875 \tAcc (train) 0.702 (val): 0.649\n",
            "Epoch: 44 \t Loss (train): 0.699 (val): 0.875 \tAcc (train) 0.703 (val): 0.647\n",
            "Epoch: 45 \t Loss (train): 0.696 (val): 0.874 \tAcc (train) 0.704 (val): 0.648\n",
            "Epoch: 46 \t Loss (train): 0.694 (val): 0.873 \tAcc (train) 0.706 (val): 0.649\n",
            "Epoch: 47 \t Loss (train): 0.692 (val): 0.872 \tAcc (train) 0.706 (val): 0.648\n",
            "Epoch: 48 \t Loss (train): 0.690 (val): 0.873 \tAcc (train) 0.707 (val): 0.649\n",
            "Epoch: 49 \t Loss (train): 0.689 (val): 0.872 \tAcc (train) 0.708 (val): 0.649\n",
            "Epoch: 50 \t Loss (train): 0.686 (val): 0.872 \tAcc (train) 0.709 (val): 0.648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BuBBhXl9q6",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL7fH9Fbd8Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metrics = evaluate(params, training_eval, subkey)\n",
        "val_metrics = evaluate(params, val_generator, subkey)\n",
        "test_metrics = evaluate(params, test_generator, subkey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dytbd3BdlKNu",
        "colab_type": "code",
        "outputId": "97ba383a-245c-42d5-8caa-30aff6ac0465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "print(\"Train metrics: \\n{}\".format(train_metrics))\n",
        "print(\"Val metrics: \\n{}\".format(val_metrics))\n",
        "print(\"Test metrics: \\n{}\".format(test_metrics))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metrics: \n",
            "{'acc': DeviceArray(0.70851755, dtype=float32), 'loss': DeviceArray(0.6860858, dtype=float32)}\n",
            "Val metrics: \n",
            "{'acc': DeviceArray(0.6483153, dtype=float32), 'loss': DeviceArray(0.8718766, dtype=float32)}\n",
            "Test metrics: \n",
            "{'acc': DeviceArray(0.6515018, dtype=float32), 'loss': DeviceArray(0.8619747, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN2ViIZIteEY",
        "colab_type": "text"
      },
      "source": [
        "### Using pre-trained embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBRvlYB4towY",
        "colab_type": "code",
        "outputId": "e3244ea6-c0d8-40a7-9f97-f184faabc7f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# download glove embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "print('Indexing word vectors.')\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.300d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = onp.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found {} word vectors.'.format(len(embeddings_index)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-22 22:12:37--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-22 22:12:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-22 22:12:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2       31%[=====>              ] 262.42M  2.11MB/s    eta 3m 54s ^C\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvK8Q3Fv52m",
        "colab_type": "code",
        "outputId": "b455cc64-09e1-4aad-ba2c-55ddae01372a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# iterate through each word in the vocabularly\n",
        "# if there exists a word embedding insert it,\n",
        "# else use a sample from a random normal\n",
        "matrix_len = len(vocab)\n",
        "weights_matrix = onp.zeros((matrix_len, 300))\n",
        "words_notfound = 0\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    try: \n",
        "        weights_matrix[i] = embeddings_index[word]\n",
        "        \n",
        "    # if there is no embedding for the given word\n",
        "    # create vector sampled from random normal\n",
        "\n",
        "    # a advantage of updating the weights of the\n",
        "    # embedding layer as part of training, is that\n",
        "    # an embedding will be learnt for these words \n",
        "    except KeyError:\n",
        "\n",
        "        weights_matrix[i] = onp.random.normal(scale=0.5, size=(300, ))\n",
        "        words_notfound += 1\n",
        "\n",
        "# clean up\n",
        "#del embeddings_index\n",
        "\n",
        "print(\"Words not found: {}\".format(words_notfound))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words not found: 441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cue-fip-7fSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _forward(inputs, rng, is_training=True):\n",
        "\n",
        "  # create mask to adjust for padding\n",
        "  mask = jnp.where(inputs==0,0,1)\n",
        "\n",
        "  # define embedding layer\n",
        "  embed = hk.Embed(len(vocab), embed_dim=300, embedding_matrix=jnp.asarray(weights_matrix))\n",
        "\n",
        "  # pass through network\n",
        "  x = embed(inputs)\n",
        "  x = sum_pooling(x, mask)\n",
        "  x = hk.Linear(128)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(64)(x)\n",
        "  x = jax.nn.relu(x)\n",
        "  x = dropout(rng, 0.5, x, is_training)\n",
        "  x = hk.Linear(5)(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2yrl61r9wiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_list, treedef = jax.tree_flatten(params)\n",
        "factor_list = [onp.array(factor) for factor in param_list]\n",
        "coef = [1./20, 1, 1, 1, 1, 1, 1]\n",
        "for p, c in zip(factor_list, coef):\n",
        "  p.fill(c)\n",
        "factor_list = [jnp.asarray(p) for p in factor_list]\n",
        "factor_tree = jax.tree_unflatten(treedef, factor_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47TtkrGN-6hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is taken from the jax.experimental.optix source\n",
        "##\n",
        "from typing import Any, Callable, NamedTuple, Sequence, Tuple, Union\n",
        "\n",
        "OptState = NamedTuple  # Optimizer state is a (possibly empty) namedtuple.\n",
        "Params = Any  # Parameters are nests of `jnp.ndarrays`.\n",
        "Updates = Params  # Gradient updates are of the same type as parameters.\n",
        "\n",
        "InitFn = Callable[[Params], Union[OptState, Sequence[OptState]]]\n",
        "UpdateFn = Callable[[Updates, OptState], Tuple[Updates, OptState]]\n",
        "\n",
        "\n",
        "class InitUpdate(NamedTuple):\n",
        "  \"\"\"Optix optimizers consists of a pair of functions: (initialiser, update).\"\"\"\n",
        "  init: InitFn\n",
        "  update: UpdateFn\n",
        "##\n",
        "\n",
        "# define a function that rescales gradient updates on a parameter level\n",
        "class ScaleParams(NamedTuple):\n",
        "  \"\"\"The scale transformation is stateless.\"\"\"\n",
        "\n",
        "\n",
        "def scale(factor_tree) -> InitUpdate:\n",
        "  \"\"\"Scale updates by some fixed scalar `step_size`.\n",
        "  Args:\n",
        "    step_size: a scalar corresponding to a fixed scaling factor for updates.\n",
        "  Returns:\n",
        "    An (init_fn, update_fn) tuple.\n",
        "  \"\"\"\n",
        "\n",
        "  def init_fn(_):\n",
        "    return ScaleParams()\n",
        "\n",
        "  def update_fn(updates, state):\n",
        "    updates = jax.tree_multimap(lambda x, y: x*y, updates, factor_tree)\n",
        "    return updates, state\n",
        "\n",
        "  return InitUpdate(init_fn, update_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrWPnmGB_W1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer():\n",
        "  \n",
        "  return optix.chain(optix.adam(2e-4),\n",
        "                     optix.scale_by_schedule(lr_schedule),\n",
        "                     scale(factor_tree))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-gN81_7yFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert into pure functions\n",
        "net = hk.transform(_forward)\n",
        "\n",
        "# initialise parameters (need sample inputs for this)\n",
        "sample_input, _ = next(iter(training_generator))\n",
        "params = net.init(jax.random.PRNGKey(42), sample_input, subkey)\n",
        "\n",
        "opt_state = make_optimizer().init(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px8k9g_R_phu",
        "colab_type": "code",
        "outputId": "2683f93d-4574-44f1-8f0e-50786e24e795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        }
      },
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "for step in range(50):\n",
        "  \n",
        "  for b_idx, batch in enumerate(training_generator):\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state = update(params, opt_state, batch, subkey)\n",
        "\n",
        "  # dont need a new random key for evalulation\n",
        "  train_metrics = evaluate(params, training_eval, subkey)\n",
        "  train_accuracy, train_loss = train_metrics[\"acc\"], train_metrics[\"loss\"]\n",
        "  train_accuracy, train_loss = jax.device_get((train_accuracy,\n",
        "                                               train_loss))\n",
        "  \n",
        "  val_metrics = evaluate(params, val_generator, subkey)\n",
        "  val_accuracy, val_loss = val_metrics[\"acc\"], val_metrics[\"loss\"]\n",
        "  val_accuracy, val_loss = jax.device_get((val_accuracy,\n",
        "                                               val_loss))\n",
        "  \n",
        "  print((\"Epoch: {} \\t Loss (train): {:.3f} (val): {:.3f} \\t\" +\n",
        "              \"Acc (train) {:.3f} (val): {:.3f}\").format(step + 1,\n",
        "                            train_loss, val_loss, train_accuracy, \n",
        "                            val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \t Loss (train): 1.045 (val): 1.055 \tAcc (train) 0.576 (val): 0.569\n",
            "Epoch: 2 \t Loss (train): 0.975 (val): 0.987 \tAcc (train) 0.596 (val): 0.589\n",
            "Epoch: 3 \t Loss (train): 0.949 (val): 0.966 \tAcc (train) 0.601 (val): 0.593\n",
            "Epoch: 4 \t Loss (train): 0.927 (val): 0.947 \tAcc (train) 0.609 (val): 0.600\n",
            "Epoch: 5 \t Loss (train): 0.912 (val): 0.936 \tAcc (train) 0.611 (val): 0.602\n",
            "Epoch: 6 \t Loss (train): 0.897 (val): 0.924 \tAcc (train) 0.621 (val): 0.611\n",
            "Epoch: 7 \t Loss (train): 0.888 (val): 0.918 \tAcc (train) 0.622 (val): 0.613\n",
            "Epoch: 8 \t Loss (train): 0.878 (val): 0.912 \tAcc (train) 0.628 (val): 0.617\n",
            "Epoch: 9 \t Loss (train): 0.868 (val): 0.905 \tAcc (train) 0.632 (val): 0.620\n",
            "Epoch: 10 \t Loss (train): 0.857 (val): 0.899 \tAcc (train) 0.635 (val): 0.623\n",
            "Epoch: 11 \t Loss (train): 0.849 (val): 0.893 \tAcc (train) 0.640 (val): 0.627\n",
            "Epoch: 12 \t Loss (train): 0.841 (val): 0.889 \tAcc (train) 0.643 (val): 0.627\n",
            "Epoch: 13 \t Loss (train): 0.836 (val): 0.887 \tAcc (train) 0.644 (val): 0.627\n",
            "Epoch: 14 \t Loss (train): 0.827 (val): 0.880 \tAcc (train) 0.649 (val): 0.632\n",
            "Epoch: 15 \t Loss (train): 0.821 (val): 0.877 \tAcc (train) 0.651 (val): 0.629\n",
            "Epoch: 16 \t Loss (train): 0.814 (val): 0.873 \tAcc (train) 0.654 (val): 0.632\n",
            "Epoch: 17 \t Loss (train): 0.809 (val): 0.871 \tAcc (train) 0.656 (val): 0.635\n",
            "Epoch: 18 \t Loss (train): 0.803 (val): 0.867 \tAcc (train) 0.658 (val): 0.637\n",
            "Epoch: 19 \t Loss (train): 0.800 (val): 0.865 \tAcc (train) 0.660 (val): 0.636\n",
            "Epoch: 20 \t Loss (train): 0.793 (val): 0.864 \tAcc (train) 0.663 (val): 0.639\n",
            "Epoch: 21 \t Loss (train): 0.789 (val): 0.862 \tAcc (train) 0.664 (val): 0.641\n",
            "Epoch: 22 \t Loss (train): 0.785 (val): 0.859 \tAcc (train) 0.666 (val): 0.640\n",
            "Epoch: 23 \t Loss (train): 0.781 (val): 0.859 \tAcc (train) 0.667 (val): 0.641\n",
            "Epoch: 24 \t Loss (train): 0.776 (val): 0.853 \tAcc (train) 0.671 (val): 0.642\n",
            "Epoch: 25 \t Loss (train): 0.774 (val): 0.855 \tAcc (train) 0.671 (val): 0.644\n",
            "Epoch: 26 \t Loss (train): 0.768 (val): 0.851 \tAcc (train) 0.674 (val): 0.647\n",
            "Epoch: 27 \t Loss (train): 0.767 (val): 0.851 \tAcc (train) 0.673 (val): 0.645\n",
            "Epoch: 28 \t Loss (train): 0.762 (val): 0.848 \tAcc (train) 0.677 (val): 0.648\n",
            "Epoch: 29 \t Loss (train): 0.761 (val): 0.848 \tAcc (train) 0.677 (val): 0.646\n",
            "Epoch: 30 \t Loss (train): 0.756 (val): 0.845 \tAcc (train) 0.679 (val): 0.648\n",
            "Epoch: 31 \t Loss (train): 0.755 (val): 0.846 \tAcc (train) 0.679 (val): 0.648\n",
            "Epoch: 32 \t Loss (train): 0.752 (val): 0.844 \tAcc (train) 0.681 (val): 0.648\n",
            "Epoch: 33 \t Loss (train): 0.749 (val): 0.845 \tAcc (train) 0.682 (val): 0.649\n",
            "Epoch: 34 \t Loss (train): 0.747 (val): 0.843 \tAcc (train) 0.683 (val): 0.650\n",
            "Epoch: 35 \t Loss (train): 0.744 (val): 0.843 \tAcc (train) 0.683 (val): 0.651\n",
            "Epoch: 36 \t Loss (train): 0.742 (val): 0.842 \tAcc (train) 0.684 (val): 0.651\n",
            "Epoch: 37 \t Loss (train): 0.741 (val): 0.840 \tAcc (train) 0.686 (val): 0.651\n",
            "Epoch: 38 \t Loss (train): 0.738 (val): 0.840 \tAcc (train) 0.686 (val): 0.652\n",
            "Epoch: 39 \t Loss (train): 0.736 (val): 0.838 \tAcc (train) 0.687 (val): 0.653\n",
            "Epoch: 40 \t Loss (train): 0.735 (val): 0.838 \tAcc (train) 0.688 (val): 0.654\n",
            "Epoch: 41 \t Loss (train): 0.731 (val): 0.837 \tAcc (train) 0.690 (val): 0.655\n",
            "Epoch: 42 \t Loss (train): 0.731 (val): 0.838 \tAcc (train) 0.690 (val): 0.653\n",
            "Epoch: 43 \t Loss (train): 0.728 (val): 0.836 \tAcc (train) 0.691 (val): 0.654\n",
            "Epoch: 44 \t Loss (train): 0.727 (val): 0.836 \tAcc (train) 0.691 (val): 0.653\n",
            "Epoch: 45 \t Loss (train): 0.726 (val): 0.837 \tAcc (train) 0.692 (val): 0.655\n",
            "Epoch: 46 \t Loss (train): 0.724 (val): 0.836 \tAcc (train) 0.692 (val): 0.655\n",
            "Epoch: 47 \t Loss (train): 0.722 (val): 0.835 \tAcc (train) 0.694 (val): 0.654\n",
            "Epoch: 48 \t Loss (train): 0.721 (val): 0.834 \tAcc (train) 0.694 (val): 0.655\n",
            "Epoch: 49 \t Loss (train): 0.720 (val): 0.834 \tAcc (train) 0.694 (val): 0.655\n",
            "Epoch: 50 \t Loss (train): 0.719 (val): 0.834 \tAcc (train) 0.695 (val): 0.655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dciV4-5d_r7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_metrics = evaluate(params, training_eval, subkey)\n",
        "val_metrics = evaluate(params, val_generator, subkey)\n",
        "test_metrics = evaluate(params, test_generator, subkey)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bt8zjVlC9eb",
        "colab_type": "code",
        "outputId": "b3f5f93d-9efb-4051-8689-21d8e77ab9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "print(\"Train metrics: \\n{}\".format(train_metrics))\n",
        "print(\"Val metrics: \\n{}\".format(val_metrics))\n",
        "print(\"Test metrics: \\n{}\".format(test_metrics))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train metrics: \n",
            "{'acc': DeviceArray(0.69457257, dtype=float32), 'loss': DeviceArray(0.7188689, dtype=float32)}\n",
            "Val metrics: \n",
            "{'acc': DeviceArray(0.65493673, dtype=float32), 'loss': DeviceArray(0.83389586, dtype=float32)}\n",
            "Test metrics: \n",
            "{'acc': DeviceArray(0.65238285, dtype=float32), 'loss': DeviceArray(0.8309346, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
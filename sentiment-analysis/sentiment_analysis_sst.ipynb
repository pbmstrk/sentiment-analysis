{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COyy_lTb4bSn"
   },
   "source": [
    "# Sentiment Analysis (Stanford Sentiment Treebank)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4r-cR8CTCMC"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "GRXJJq556aeE",
    "outputId": "327b1ed0-7a04-4888-def3-576c8b29175d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 409kB 2.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.8MB 13.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 276kB 32.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 829kB 24.7MB/s \n",
      "\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLBtQm_0qabd"
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn.utils.rnn import (pad_sequence, pack_padded_sequence, \n",
    "                                  pad_packed_sequence)\n",
    "\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqBqEH2l5ARf"
   },
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "The first model is based on an architecture presented in the paper, [Bag of Tricks for Efficient Text Classification](https://www.aclweb.org/anthology/E17-2068/). In particular, we use a Multi-Layer Perceptron (MLP) to predict sentiment using an aggregated representation of the input. The input is tokenized into individual words, and each word is then represented by its embedding. Inputs can be of different length, thus, the embeddings are aggregated by the use of a pooling function (often mean-pooling - that is, simply the element-wise mean) to create a fixed-size representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation differs slightly from the presentation in the paper, since we make use of pre-trained word embeddings and do not implement a hierarchical softmax. Regarding the use of embeddings, there are two options: (1) keep the pre-trained embeddings fixed during training, and (2) include the embeddings in the optimisation. In our implementation we choose to optimise the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RVu0arV47Et"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UjTuj0n1xIw"
   },
   "outputs": [],
   "source": [
    "# use a named-tuple to store data examples\n",
    "Example = namedtuple(\"Example\", [\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2oKrI4LQTMW"
   },
   "outputs": [],
   "source": [
    "class SST_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        element = self.dataset[idx] \n",
    "        X = element.text\n",
    "        Y = element.label\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAQRDySoQTSR"
   },
   "outputs": [],
   "source": [
    "class SSTDataModuleBase(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.PAD_token = 0\n",
    "        self.UNK_token = 1\n",
    "        self.SOS_token = 2\n",
    "        self.EOS_token = 3\n",
    "\n",
    "        self.targetEncoding = {'negative': 0, 'positive': 1}\n",
    "        \n",
    "    \n",
    "    def _format_data(self, dataset):\n",
    "        \n",
    "        tokenized_dataset = []\n",
    "        for element in dataset:\n",
    "            encoding = self._tokenize(element)\n",
    "            tokenized_dataset.append(Example(text=encoding[0], label=encoding[1]))\n",
    "\n",
    "        return tokenized_dataset\n",
    "\n",
    "\n",
    "    def embedding_matrix(self):\n",
    "\n",
    "        glove = torchtext.vocab.GloVe(name='6B', dim=300, \n",
    "                                      unk_init = torch.Tensor.normal_)\n",
    "        matrix_len = len(self._wordlist)\n",
    "        weights_matrix = np.zeros((matrix_len, 300))\n",
    "\n",
    "        for i, word in enumerate(self._wordlist):\n",
    "            try: \n",
    "                weights_matrix[i] = glove.vectors[glove.stoi[word]]\n",
    "            except KeyError:\n",
    "                weights_matrix[i] = np.random.normal(scale=0.5, size=(300, ))\n",
    "\n",
    "        return weights_matrix   \n",
    "\n",
    "    @staticmethod\n",
    "    def _flatten(lst):\n",
    "        return [item for sublist in lst for item in sublist]\n",
    "        \n",
    "    def _build_vocab(self, data):\n",
    "        vocab_counter = Counter(self._flatten([example.text for example in data]))\n",
    "        return vocab_counter\n",
    "\n",
    "\n",
    "    def _build_encoding(self, vocab_count, min_freq=3):\n",
    "\n",
    "        self._wordlist = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "        self.encoding = {}\n",
    "\n",
    "        svocabCount = {k: v for k, v in reversed(sorted(vocab_count.items(), \n",
    "                                        key=lambda item: item[1]))}\n",
    "\n",
    "        for word in svocabCount:\n",
    "            if svocabCount[word] >= min_freq:\n",
    "                self._wordlist.append(word)\n",
    "        self.encoding.update({tok: i for i, tok in enumerate(self._wordlist)})\n",
    "        \n",
    "    def _tokenize(self, element):\n",
    "\n",
    "        text = (torch.tensor([self.SOS_token] + \n",
    "          [self.encoding.get(word, self.UNK_token) for word in element.text] + \n",
    "          [self.EOS_token]))\n",
    "        label = torch.tensor(self.targetEncoding[element.label])\n",
    "\n",
    "        return text, label  \n",
    "\n",
    "    def setup(self, stage=None, min_freq=3):\n",
    "\n",
    "        TEXT = torchtext.data.Field(tokenize='spacy', lower=True)\n",
    "        LABEL = torchtext.data.Field(sequential=False)\n",
    "        \n",
    "        train_data, val_data, test_data = torchtext.datasets.SST.splits(TEXT, \n",
    "          LABEL, filter_pred=lambda ex: ex.label != 'neutral', train_subtrees=True)\n",
    "        \n",
    "      \n",
    "        vocab_counter = self._build_vocab(train_data)\n",
    "        self._build_encoding(vocab_counter, min_freq)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.sst_train = SST_Dataset(self._format_data(train_data))\n",
    "            self.sst_val = SST_Dataset(self._format_data(val_data))\n",
    "\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.sst_test = SST_Dataset(self._format_data(test_data))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ7TL-UhVjYd"
   },
   "outputs": [],
   "source": [
    "class SSTDataModuleMLP(SSTDataModuleBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _collate_fn(batch):\n",
    "        # get data and targets from batch\n",
    "        data = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "        lengths = [len(el) for el in data]\n",
    "        offsets = np.cumsum(lengths)\n",
    "        offsets = np.concatenate([[0], offsets[:-1]])\n",
    "\n",
    "        return (torch.LongTensor(torch.cat(data).long()), \n",
    "                torch.Tensor(targets).float(), \n",
    "                torch.LongTensor(offsets))\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.sst_train, batch_size=64, \n",
    "                              collate_fn=self._collate_fn, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.sst_val, batch_size=64, \n",
    "                              collate_fn=self._collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.sst_test, batch_size=64,\n",
    "                              collate_fn=self._collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bYhcmGODOZ2"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "id": "CRXfntQODbGv",
    "outputId": "1f1d6ede-68f8-4ed5-8748-c6e04340e05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading trainDevTestTrees_PTB.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:02<00:00, 365kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "ds = SSTDataModuleMLP()\n",
    "ds.setup(min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrXKE6qy5587"
   },
   "outputs": [],
   "source": [
    "class FastText(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_mat=None):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, 300, mode=\"mean\")\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(300, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        inputs, _, offsets = batch\n",
    "        # inputs: [SUM(SEQ_LENGTHS)]\n",
    "\n",
    "        x = self.embedding(inputs, offsets)\n",
    "        # x: [BATCH_SIZE, EMBED_DIM]\n",
    "\n",
    "        x = self.fc(x).squeeze()\n",
    "        # x: [BATCH_SIZE]\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y, offsets = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y, offsets = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "      \n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']\n",
    "                }\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "UOALKgeUuqrx",
    "outputId": "b54d409e-ffa7-4833-a6cd-bd185e369f76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:17, 1.97MB/s]                           \n",
      "100%|█████████▉| 399896/400000 [00:38<00:00, 9750.45it/s]"
     ]
    }
   ],
   "source": [
    "embed_mat = ds.embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "id": "f4ACLOnlQShJ",
    "outputId": "2b1546ec-1f4e-4a60-82ba-1cb4f3914e35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type         | Params\n",
      "-------------------------------------------\n",
      "0 | embedding | EmbeddingBag | 4 M   \n",
      "1 | fc        | Sequential   | 77 K  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7603\t Validation loss: 0.4828\n",
      "Epoch 0:\t Train loss: 0.4761\n",
      "Epoch 1:\t Validation acc: 0.7649\t Validation loss: 0.4715\n",
      "Epoch 1:\t Train loss: 0.3741\n",
      "Epoch 2:\t Validation acc: 0.7661\t Validation loss: 0.4628\n",
      "Epoch 2:\t Train loss: 0.3625\n",
      "Epoch 3:\t Validation acc: 0.7741\t Validation loss: 0.4580\n",
      "Epoch 3:\t Train loss: 0.3554\n",
      "Epoch 4:\t Validation acc: 0.7718\t Validation loss: 0.4560\n",
      "Epoch 4:\t Train loss: 0.3511\n",
      "Epoch 5:\t Validation acc: 0.7683\t Validation loss: 0.4534\n",
      "Epoch 5:\t Train loss: 0.3469\n",
      "Epoch 6:\t Validation acc: 0.7729\t Validation loss: 0.4525\n",
      "Epoch 6:\t Train loss: 0.3428\n",
      "Epoch 7:\t Validation acc: 0.7695\t Validation loss: 0.4508\n",
      "Epoch 7:\t Train loss: 0.3387\n",
      "Epoch 8:\t Validation acc: 0.7729\t Validation loss: 0.4500\n",
      "Epoch 8:\t Train loss: 0.3353\n",
      "Epoch 9:\t Validation acc: 0.7741\t Validation loss: 0.4482\n",
      "Epoch 9:\t Train loss: 0.3315\n",
      "Epoch 10:\t Validation acc: 0.7706\t Validation loss: 0.4473\n",
      "Epoch 10:\t Train loss: 0.3288\n",
      "Epoch 11:\t Validation acc: 0.7798\t Validation loss: 0.4469\n",
      "Epoch 11:\t Train loss: 0.3252\n",
      "Epoch 12:\t Validation acc: 0.7752\t Validation loss: 0.4458\n",
      "Epoch 12:\t Train loss: 0.3222\n",
      "Epoch 13:\t Validation acc: 0.7867\t Validation loss: 0.4460\n",
      "Epoch 13:\t Train loss: 0.3191\n",
      "Epoch 14:\t Validation acc: 0.7683\t Validation loss: 0.4427\n",
      "Epoch 14:\t Train loss: 0.3163\n",
      "Epoch 15:\t Validation acc: 0.7741\t Validation loss: 0.4428\n",
      "Epoch 15:\t Train loss: 0.3137\n",
      "Epoch 16:\t Validation acc: 0.7683\t Validation loss: 0.4416\n",
      "Epoch 16:\t Train loss: 0.3106\n",
      "Epoch 17:\t Validation acc: 0.7775\t Validation loss: 0.4419\n",
      "Epoch 17:\t Train loss: 0.3085\n",
      "Epoch 18:\t Validation acc: 0.7867\t Validation loss: 0.4437\n",
      "Epoch 18:\t Train loss: 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\t Validation acc: 0.7856\t Validation loss: 0.4424\n",
      "Epoch 19:\t Train loss: 0.3031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fasttext(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=30, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "SfYv7tahQTJO",
    "outputId": "5d128e0d-68b7-4231-dbf7-c29d5a79b5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.7985, device='cuda:0'),\n",
      " 'test_loss': tensor(0.4242, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_results= trainer.test(model, \n",
    "                           ds.test_dataloader(), \n",
    "                           ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEO3rkANztG2"
   },
   "source": [
    "## 2. LSTM\n",
    "\n",
    "The appeal of the `FastText` model is its simplicity, however, it suffers from some inherent disadvantages. In particular, the model does not consider word order. The [paper](https://www.aclweb.org/anthology/E17-2068/) addresses this issue by including additional n-gram features to capture local word ordering. An alternative approach is to use a different model architecture designed for sequential data, recurrent neural networks. We now implement a single-layer LSTM for the sentiment classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81VY4WvQz1VJ"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0C6zGr6zvRn"
   },
   "outputs": [],
   "source": [
    "class SSTDataModuleLSTM(SSTDataModuleBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _collate_fn(batch):\n",
    "        # get inputs and targets\n",
    "        data = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "\n",
    "        # to be able to pack sequences later on, need\n",
    "        # the original sequence lengths\n",
    "        seqlengths = [len(el) for el in data]\n",
    "    \n",
    "        # pad the sequences\n",
    "        x = pad_sequence(data, batch_first=True)\n",
    "\n",
    "        return (x, torch.Tensor(targets).float(), \n",
    "                      seqlengths)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.sst_train, batch_size=64, \n",
    "                              collate_fn=self._collate_fn, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.sst_val, batch_size=64, \n",
    "                              collate_fn=self._collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.sst_test, batch_size=64,\n",
    "                              collate_fn=self._collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7LEuRqHPuy8"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO6bwrWOPvee"
   },
   "outputs": [],
   "source": [
    "class lstm(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_mat):\n",
    "    \n",
    "        super().__init__()    \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "        self.lstm = nn.LSTM(300, 256, num_layers = 1, batch_first = True)\n",
    "        self.linear1 = nn.Linear(256, 64)\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        inputs, _, seqlengths = batch\n",
    "        # inputs: [BATCH_SIZE, LONGEST_SEQ]\n",
    "    \n",
    "        embeds = self.embedding(inputs.long())\n",
    "        # embeds: [BATCH_SIZE, LONGEST_SEQ, EMBED_DIM]\n",
    "\n",
    "        embeds = self.dropout(embeds)\n",
    "\n",
    "        inputs = pack_padded_sequence(embeds, seqlengths, \n",
    "              enforce_sorted=False, batch_first=True)\n",
    "        # inputs: [SUM(SEQ_LENGTHS), EMBED_DIM)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(inputs)\n",
    "        # packed_outputs: [SUM(SEQ_LENGTHS), LSTM_OUT]\n",
    "        # hidden: [1, BATCH_SIZE, LSTM_OUT]\n",
    "\n",
    "        lastState = hidden[-1]\n",
    "        # lastState: [BATCH_SIZE, LSTM_OUT]\n",
    "\n",
    "        output = self.dropout(F.relu(self.linear1(lastState)))\n",
    "        output = self.linear2(output).squeeze()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y , seqlengths = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y, offsets = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YesMTW3RR5wo"
   },
   "outputs": [],
   "source": [
    "ds = SSTDataModuleLSTM()\n",
    "ds.setup(min_freq=3)\n",
    "embed_mat = ds.embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "Dqly0of7QqGd",
    "outputId": "441dea02-f925-47d3-8009-c7f52e7208f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 4 M   \n",
      "1 | lstm      | LSTM      | 571 K \n",
      "2 | linear1   | Linear    | 16 K  \n",
      "3 | linear2   | Linear    | 65    \n",
      "4 | dropout   | Dropout   | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7844\t Validation loss: 0.4569\n",
      "Epoch 0:\t Train loss: 0.4817\n",
      "Epoch 1:\t Validation acc: 0.7913\t Validation loss: 0.4363\n",
      "Epoch 1:\t Train loss: 0.4019\n",
      "Epoch 2:\t Validation acc: 0.7936\t Validation loss: 0.4351\n",
      "Epoch 2:\t Train loss: 0.3798\n",
      "Epoch 3:\t Validation acc: 0.7901\t Validation loss: 0.4441\n",
      "Epoch 3:\t Train loss: 0.3618\n",
      "Epoch 4:\t Validation acc: 0.8062\t Validation loss: 0.4168\n",
      "Epoch 4:\t Train loss: 0.3452\n",
      "Epoch 5:\t Validation acc: 0.7970\t Validation loss: 0.4212\n",
      "Epoch 5:\t Train loss: 0.3329\n",
      "Epoch 6:\t Validation acc: 0.7970\t Validation loss: 0.4148\n",
      "Epoch 6:\t Train loss: 0.3191\n",
      "Epoch 7:\t Validation acc: 0.8131\t Validation loss: 0.3925\n",
      "Epoch 7:\t Train loss: 0.3077\n",
      "Epoch 8:\t Validation acc: 0.8085\t Validation loss: 0.4141\n",
      "Epoch 8:\t Train loss: 0.2986\n",
      "Epoch 9:\t Validation acc: 0.8108\t Validation loss: 0.4033\n",
      "Epoch 9:\t Train loss: 0.2882\n",
      "Epoch 10:\t Validation acc: 0.7924\t Validation loss: 0.5580\n",
      "Epoch 10:\t Train loss: 0.2779\n",
      "Epoch 11:\t Validation acc: 0.8096\t Validation loss: 0.4059\n",
      "Epoch 11:\t Train loss: 0.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\t Validation acc: 0.8108\t Validation loss: 0.4320\n",
      "Epoch 12:\t Train loss: 0.2635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=5,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=30, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "cGj6lcacRAB9",
    "outputId": "8f41210e-2165-4218-e4db-5d94412c5dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.8303, device='cuda:0'),\n",
      " 'test_loss': tensor(0.3620, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model, \n",
    "                            ds.test_dataloader(), \n",
    "                            ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0sWlXlHWS6i"
   },
   "source": [
    "## Attention-LSTM\n",
    "\n",
    "In the LSTM-model presented above, the dense layer receives the last hidden state as input. This implies that all the information required to make a prediction must be contained in the last hidden state, thus creating a bottleneck of information.\n",
    "\n",
    "Rather than using the last hidden state as input, we can compute a representation through the use of weighted sum of all the hidden states by using an attention mechanism. In that way, we can \"focus\" on words that heavily influence the sentiment of the sentence.\n",
    "\n",
    "The model implemented is the word-sequence and word-attention model presented in [Hierarchical Attention Networks for Document Classification](https://www.aclweb.org/anthology/N16-1174/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYeRW9dfW1D2"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, att_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(2*hidden_size, att_dim)\n",
    "        self.u_w = nn.Linear(att_dim, 1)\n",
    "\n",
    "    def forward(self, outputs, mask):\n",
    "\n",
    "        outputs = outputs.permute(0,2,1)\n",
    "        # outputs: [BATCH, LSTM_OUT, LONGEST_SEQ]\n",
    "\n",
    "        # compute u_{it} representations of each of the hidden \n",
    "        u_it = torch.einsum('ki,bij->bkj', self.w.weight, outputs)\n",
    "        # u_it: [BATCH, ATTN_OUT, LONGEST_SEQ]\n",
    "        \n",
    "        # TODO: 'ij, bjk -> bik' - don't need unsqueeze then.\n",
    "        \n",
    "        # compute alpha\n",
    "        alpha = torch.einsum('ij, bjk->bk', self.u_w.weight, u_it).unsqueeze(1)\n",
    "        # alpha: [BATCH, LONGEST_SEQ]\n",
    "\n",
    "        # use mask\n",
    "        alpha = alpha.masked_fill(mask == 0, -1e-10)\n",
    "\n",
    "        return F.softmax(alpha, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihhMGn-2WVON"
   },
   "outputs": [],
   "source": [
    "class lstmAttn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_mat):\n",
    "    \n",
    "        super().__init__()    \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "        self.attention = Attention(256, 200)\n",
    "        self.lstm = nn.LSTM(300, 256, bidirectional=True, batch_first = True)\n",
    "        self.fc1 = nn.Linear(256*2, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.embed_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def _create_mask(self, inputs):\n",
    "        mask = (inputs != 0)\n",
    "        return mask.unsqueeze(1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        inputs, _, seqlengths = batch\n",
    "        # inputs: [BATCH_SIZE, LONGEST_SEQ]\n",
    "\n",
    "        mask = self._create_mask(inputs)\n",
    "        # mask: [BATCH_SIZE, 1, LONGEST_SEQ]\n",
    "\n",
    "        embeds = self.embedding(inputs.long())\n",
    "        # embeds: [BATCH_SIZE, LONGEST_SEQ, EMBED_DIM]\n",
    "\n",
    "        embeds = self.embed_dropout(embeds)\n",
    "\n",
    "        inputs = pack_padded_sequence(embeds, seqlengths, \n",
    "              enforce_sorted=False, batch_first=True)\n",
    "        # inputs: [SUM(SEQ_LENGTHS), EMBED_DIM)\n",
    "     \n",
    "        packed_outputs, (hidden, cell) = self.lstm(inputs)\n",
    "        # packed_outputs: [SUM(SEQ_LENGTHS), N_DIR * LSTM_OUT]\n",
    "        # hidden: [N_DIR * N_LAYERS, BATCH_SIZE, LSTM_OUT]\n",
    "\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # outputs: [BATCH_SIZE, LONGEST_SEQ, N_DIR * LSTM_OUT]\n",
    "\n",
    "        a = self.attention(outputs, mask)\n",
    "        # a: [BATCH_SIZE, 1, LONGEST_SEQ]\n",
    "        \n",
    "        # test here\n",
    "        # context = torch.einsum('bij, bjk -> bik', a, outputs)\n",
    "        # in fact could get rid of i here to avoid squeeze\n",
    "        \n",
    "        # maybe we can get rid of the unsqueeze in mask\n",
    "        \n",
    "        \n",
    "        weighted = torch.bmm(a, outputs).squeeze(1)\n",
    "        # weighted: [BATCH_SIZE, N_DIR * LSTM_OUT]\n",
    "\n",
    "        # linear layer\n",
    "        output = self.dropout(F.relu(self.fc1(weighted)))\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y , seqlengths = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y, offsets = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "      \n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']\n",
    "                }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "dDYF_6BsXFjG",
    "outputId": "9538c9d5-0df6-4770-b466-dfbc513d900e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name          | Type      | Params\n",
      "--------------------------------------------\n",
      "0 | embedding     | Embedding | 4 M   \n",
      "1 | attention     | Attention | 102 K \n",
      "2 | lstm          | LSTM      | 1 M   \n",
      "3 | fc1           | Linear    | 32 K  \n",
      "4 | fc2           | Linear    | 65    \n",
      "5 | dropout       | Dropout   | 0     \n",
      "6 | embed_dropout | Dropout   | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7924\t Validation loss: 0.4435\n",
      "Epoch 0:\t Train loss: 0.4136\n",
      "Epoch 1:\t Validation acc: 0.8016\t Validation loss: 0.4278\n",
      "Epoch 1:\t Train loss: 0.3347\n",
      "Epoch 2:\t Validation acc: 0.8039\t Validation loss: 0.4147\n",
      "Epoch 2:\t Train loss: 0.3071\n",
      "Epoch 3:\t Validation acc: 0.8280\t Validation loss: 0.4060\n",
      "Epoch 3:\t Train loss: 0.2848\n",
      "Epoch 4:\t Validation acc: 0.8188\t Validation loss: 0.4101\n",
      "Epoch 4:\t Train loss: 0.2652\n",
      "Epoch 5:\t Validation acc: 0.8268\t Validation loss: 0.4135\n",
      "Epoch 5:\t Train loss: 0.2480\n",
      "Epoch 6:\t Validation acc: 0.8165\t Validation loss: 0.4358\n",
      "Epoch 6:\t Train loss: 0.2311\n",
      "Epoch 7:\t Validation acc: 0.8154\t Validation loss: 0.4331\n",
      "Epoch 7:\t Train loss: 0.2156\n",
      "Epoch 8:\t Validation acc: 0.8211\t Validation loss: 0.4381\n",
      "Epoch 8:\t Train loss: 0.2029\n",
      "Epoch 9:\t Validation acc: 0.8326\t Validation loss: 0.4462\n",
      "Epoch 9:\t Train loss: 0.1888\n",
      "Epoch 10:\t Validation acc: 0.8337\t Validation loss: 0.4567\n",
      "Epoch 10:\t Train loss: 0.1776\n",
      "Epoch 11:\t Validation acc: 0.8326\t Validation loss: 0.4516\n",
      "Epoch 11:\t Train loss: 0.1667\n",
      "Epoch 12:\t Validation acc: 0.8222\t Validation loss: 0.4848\n",
      "Epoch 12:\t Train loss: 0.1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\t Validation acc: 0.8257\t Validation loss: 0.4726\n",
      "Epoch 13:\t Train loss: 0.1482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstmAttn(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=10,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=50, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "p5Lu_NPVXdMu",
    "outputId": "270da84c-07ba-44a8-bc76-7c782cefdcd1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.8413, device='cuda:0'),\n",
      " 'test_loss': tensor(0.4089, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model, \n",
    "                            ds.test_dataloader(), \n",
    "                            ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQnMIcyqT8p0"
   },
   "source": [
    "## CNN\n",
    "\n",
    "We can use a CNN for text classification as presented in [Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181/), by using convolution operations on the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jARq9K6rW11B"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu-uFOqdUI3r"
   },
   "outputs": [],
   "source": [
    "class SSTDataModuleCNN(SSTDataModuleBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _collate_fn(batch):\n",
    "        # get inputs and targets\n",
    "        data = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "    \n",
    "        # pad the sequences\n",
    "        x = pad_sequence(data, batch_first=True)\n",
    "\n",
    "        return x, torch.Tensor(targets).float()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.sst_train, batch_size=64, \n",
    "                              collate_fn=self._collate_fn, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.sst_val, batch_size=64, \n",
    "                              collate_fn=self._collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.sst_test, batch_size=64,\n",
    "                              collate_fn=self._collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umA4rsPNW32i"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWQ0f4oXUUKn"
   },
   "outputs": [],
   "source": [
    "# TODO: rename variables\n",
    "class TextCnn(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_mat=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = 300, \n",
    "                                              out_channels = 100, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in [3,4,5]\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(3 * 100, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        inputs, _ = batch\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(inputs)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat).squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "      \n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pArmGjHYU6la"
   },
   "outputs": [],
   "source": [
    "ds = SSTDataModuleCNN()\n",
    "ds.setup(min_freq=3)\n",
    "embed_mat = ds.embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "N6MBEU75VA3n",
    "outputId": "d81531dc-26dd-4ad6-9706-b935f65fa4f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | embedding | Embedding  | 4 M   \n",
      "1 | convs     | ModuleList | 360 K \n",
      "2 | fc        | Linear     | 301   \n",
      "3 | dropout   | Dropout    | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7856\t Validation loss: 0.4401\n",
      "Epoch 0:\t Train loss: 0.4202\n",
      "Epoch 1:\t Validation acc: 0.7970\t Validation loss: 0.4272\n",
      "Epoch 1:\t Train loss: 0.3189\n",
      "Epoch 2:\t Validation acc: 0.7947\t Validation loss: 0.4195\n",
      "Epoch 2:\t Train loss: 0.2942\n",
      "Epoch 3:\t Validation acc: 0.8016\t Validation loss: 0.4155\n",
      "Epoch 3:\t Train loss: 0.2745\n",
      "Epoch 4:\t Validation acc: 0.8005\t Validation loss: 0.4083\n",
      "Epoch 4:\t Train loss: 0.2552\n",
      "Epoch 5:\t Validation acc: 0.8073\t Validation loss: 0.4055\n",
      "Epoch 5:\t Train loss: 0.2400\n",
      "Epoch 6:\t Validation acc: 0.8085\t Validation loss: 0.4005\n",
      "Epoch 6:\t Train loss: 0.2254\n",
      "Epoch 7:\t Validation acc: 0.7982\t Validation loss: 0.4123\n",
      "Epoch 7:\t Train loss: 0.2115\n",
      "Epoch 8:\t Validation acc: 0.8108\t Validation loss: 0.4053\n",
      "Epoch 8:\t Train loss: 0.1997\n",
      "Epoch 9:\t Validation acc: 0.8062\t Validation loss: 0.4054\n",
      "Epoch 9:\t Train loss: 0.1899\n",
      "Epoch 10:\t Validation acc: 0.8062\t Validation loss: 0.4043\n",
      "Epoch 10:\t Train loss: 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\t Validation acc: 0.8131\t Validation loss: 0.4057\n",
      "Epoch 11:\t Train loss: 0.1732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cnn(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=5,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=30, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "rhGrdLggVEZ-",
    "outputId": "b669b7d4-5a89-4aff-9c6e-146e396405b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.8424, device='cuda:0'),\n",
      " 'test_loss': tensor(0.3687, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model, \n",
    "                            ds.test_dataloader(), \n",
    "                            ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcEMe0ARZrqr"
   },
   "source": [
    "## Recurrent Neural Filters\n",
    "\n",
    "We now present a variant of the CNN model which uses a recurrent network as a convolution filter. The embeddings are split into chunks, and an LSTM is applied to each chunk to compute a representation. The representations are then aggregated using a pooling operations. More details are in the paper, [Convolutional Neural Networks with Recurrent Neural Filters](https://www.aclweb.org/anthology/D18-1109/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kj_RUv10Zsnr"
   },
   "outputs": [],
   "source": [
    "class TimeDistributedLSTM(pl.LightningModule):\n",
    "    def __init__(self, time_axis):        \n",
    "        super(TimeDistributedLSTM, self).__init__()\n",
    "\n",
    "        self.time_axis = time_axis\n",
    "        self.lstm = nn.LSTM(300, 300, batch_first = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        time_steps = x.shape[self.time_axis]\n",
    "        embed_dim = x.shape[-1] \n",
    "        outputs = torch.zeros(x.shape[0], time_steps, embed_dim, device=self.device)\n",
    "        \n",
    "        for i in range(time_steps):\n",
    "            x_input = torch.index_select(x, dim=self.time_axis, index=torch.tensor([i], device=self.device).long()).squeeze()\n",
    "            \n",
    "            output_t, (cell_t, hidden_t) = self.lstm(x_input)\n",
    "            \n",
    "            outputs[:, i, :] = hidden_t\n",
    "           \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1XpnBZzQ3X2"
   },
   "outputs": [],
   "source": [
    "def format_conv_input(x, filter_width, sent_len):\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(sent_len - filter_width + 1):\n",
    "        chunk = x[:, i:i+filter_width, :]\n",
    "        chunk = chunk.unsqueeze(1)\n",
    "        chunks.append(chunk)\n",
    "    return torch.cat(chunks, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-r0ToV1h4ez"
   },
   "outputs": [],
   "source": [
    "class RNF(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_mat=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "\n",
    "        self.filter_width = 5\n",
    "        self.time_lstm = TimeDistributedLSTM(time_axis=1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(300, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        inputs, _ = batch\n",
    "        # inputs: [BATCH SIZE, LONGEST SEQ]\n",
    "        \n",
    "        embedded = self.embedding(inputs)       \n",
    "        # embedded: [BATCH SIZE, LONGEST SEQ, EMBED DIM]\n",
    "\n",
    "        lstm_inputs = format_conv_input(embedded, \n",
    "                                        filter_width=self.filter_width, \n",
    "                                        sent_len=embedded.shape[1])\n",
    "\n",
    "        lstm_outputs = self.time_lstm(lstm_inputs)\n",
    "        \n",
    "        lstm_outputs = F.max_pool1d(lstm_outputs.permute(0,2,1),kernel_size=lstm_outputs.shape[1]).squeeze()\n",
    "        \n",
    "        outputs = self.fc(lstm_outputs)\n",
    "        return outputs.squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "      \n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3_pfHOzUKK4"
   },
   "outputs": [],
   "source": [
    "ds = SSTDataModuleCNN()\n",
    "ds.setup(min_freq=3)\n",
    "emlbed_mat = ds.embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "5k8g27BMSS1z",
    "outputId": "d97eca50-bc78-4e9f-f0a5-c482422f6d92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | embedding | Embedding           | 4 M   \n",
      "1 | time_lstm | TimeDistributedLSTM | 722 K \n",
      "2 | fc        | Sequential          | 77 K  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7798\t Validation loss: 0.4502\n",
      "Epoch 0:\t Train loss: 0.3933\n",
      "Epoch 1:\t Validation acc: 0.8016\t Validation loss: 0.4215\n",
      "Epoch 1:\t Train loss: 0.3151\n",
      "Epoch 2:\t Validation acc: 0.8119\t Validation loss: 0.3985\n",
      "Epoch 2:\t Train loss: 0.2842\n",
      "Epoch 3:\t Validation acc: 0.8234\t Validation loss: 0.3934\n",
      "Epoch 3:\t Train loss: 0.2540\n",
      "Epoch 4:\t Validation acc: 0.8131\t Validation loss: 0.4084\n",
      "Epoch 4:\t Train loss: 0.2272\n",
      "Epoch 5:\t Validation acc: 0.8188\t Validation loss: 0.3983\n",
      "Epoch 5:\t Train loss: 0.2030\n",
      "Epoch 6:\t Validation acc: 0.8119\t Validation loss: 0.4133\n",
      "Epoch 6:\t Train loss: 0.1800\n",
      "Epoch 7:\t Validation acc: 0.8177\t Validation loss: 0.4186\n",
      "Epoch 7:\t Train loss: 0.1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\t Validation acc: 0.8188\t Validation loss: 0.4319\n",
      "Epoch 8:\t Train loss: 0.1470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNF(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=5,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=30, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "sDRCpEHBSdVl",
    "outputId": "1dca9d7c-26c7-471d-bf89-eca06ad7511a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.8451, device='cuda:0'),\n",
      " 'test_loss': tensor(0.3769, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model, \n",
    "                            ds.test_dataloader(), \n",
    "                            ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV2DLPrSZfM5"
   },
   "source": [
    "## Neural Semantic Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model is an implemenations of [Neural Semantic Encoders](https://www.aclweb.org/anthology/E17-1038/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1kPCSmAz_Or"
   },
   "outputs": [],
   "source": [
    "class NSE(pl.LightningModule):\n",
    "\n",
    "\n",
    "    def __init__(self, vocab_size,embed_mat=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        if embed_mat is not None:\n",
    "            self.embedding = self.embedding.from_pretrained(torch.from_numpy(embed_mat).float())\n",
    "\n",
    "        self.read_lstm = nn.LSTM(300, 300, batch_first = True)\n",
    "        self.write_lstm = nn.LSTM(2*300, 300, batch_first=True)\n",
    "        self.compose_layer = nn.Linear(2*300, 2*300)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(300, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def _init_hidden(self, batch_size, hidden_dim):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, batch_size, hidden_dim, device=self.device).requires_grad_(),\n",
    "                torch.zeros(1, batch_size, hidden_dim, device=self.device).requires_grad_())\n",
    "    \n",
    "    def _compose(self, o_t, m_t):\n",
    "\n",
    "        \"\"\"\n",
    "        Compose operation.\n",
    "        Eq. (4)\n",
    "        \"\"\"\n",
    "        c_t = self.compose_layer(torch.cat([o_t, m_t], dim=1))\n",
    "        # c_t: [BATCH_SIZE, 2*N_UNITS]\n",
    "        return c_t\n",
    "\n",
    "    def _read(self, M_t, x_t, hidden):\n",
    "\n",
    "        \"\"\"\n",
    "        Read operation.\n",
    "        Eq. (1)-(3)\n",
    "        \"\"\"\n",
    "        o_t, hidden = self.read_lstm(F.dropout(x_t, 0.3), hidden)\n",
    "        # o_t: [BATCH_SIZE, 1, DIM]\n",
    "\n",
    "        o_t = o_t.squeeze(1)\n",
    "        # o_t: [BATCH_SIZE, DIM]\n",
    "        \n",
    "        z_t = F.softmax(torch.einsum(\"bo,bko->bk\", o_t, M_t), dim=1)\n",
    "        m_rt = torch.einsum(\"bk,bko->bo\", z_t, M_t)\n",
    "        return o_t, m_rt, z_t, hidden\n",
    "\n",
    "    def _write(self, M_t, c_t, z_t, hidden):\n",
    "\n",
    "        batch_size = c_t.shape[0]\n",
    "        len = z_t.shape[1]\n",
    "        dim = M_t.shape[2]\n",
    "\n",
    "        h_t, hidden = self.write_lstm(F.dropout(c_t.unsqueeze(1), 0.3), hidden)\n",
    "        z_t_e_k = torch.einsum('ki,kj->kji', [torch.ones(batch_size, dim, device=self.device), z_t])\n",
    "        M_t = (1 - z_t_e_k) * M_t + torch.einsum('ki,kj->kij', [torch.ones(batch_size,len, device=self.device), h_t.squeeze(1)]) * z_t_e_k\n",
    "        return M_t, h_t, hidden\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        inputs, _, seqlengths = batch\n",
    "        # inputs: [BATCH_SIZE, LONGEST_SEQ]\n",
    "\n",
    "        embeds = self.embedding(inputs.long())\n",
    "        # embeds: [BATCH_SIZE, LONGEST_SEQ, EMBED_DIM]\n",
    "\n",
    "        M_t = embeds\n",
    "\n",
    "        all_outputs = torch.zeros(inputs.shape[0], inputs.shape[1], 300, device=self.device)\n",
    "        idx = torch.tensor(seqlengths)\n",
    "        idx = idx - 1 \n",
    "        \n",
    "        read_hidden = self._init_hidden(inputs.shape[0], 300)\n",
    "        write_hidden = self._init_hidden(inputs.shape[0], 300)\n",
    "\n",
    "        for i in range(inputs.shape[1]):\n",
    "\n",
    "            x_t = torch.index_select(embeds, 1, torch.tensor([i]).long().cuda())\n",
    "            # x_t: [BATCH_SIZE, 1, DIM]\n",
    "\n",
    "            o_t, m_rt, z_t, read_hidden = self._read(M_t, x_t, read_hidden)\n",
    "\n",
    "            c_t = self._compose(o_t, m_rt)\n",
    "\n",
    "            M_t, h_t, write_hidden = self._write(M_t, c_t, z_t, write_hidden)\n",
    "\n",
    "            all_outputs[:, i, :] = h_t.squeeze(1)\n",
    "\n",
    "        output = all_outputs[torch.arange(all_outputs.size(0)), idx]\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, \n",
    "                                     weight_decay=1e-05)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y , seqlengths = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "        return {'loss': loss, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch}:\\t Train loss: {avg_loss:.4f}\")\n",
    "        return {'avg_train_loss': avg_loss}\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y, offsets = batch\n",
    "        y_hat = self(batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        \n",
    "        # compute acc\n",
    "        preds = torch.round(torch.sigmoid(y_hat))\n",
    "        correct = (preds == y).float().sum()\n",
    "        acc = correct/len(y)\n",
    "\n",
    "        return {\"loss\": loss, \n",
    "                \"acc\": acc, \n",
    "                \"batch_size\": len(y)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "      \n",
    "        total = sum([x['batch_size'] for x in outputs])\n",
    "        avg_loss = sum([x['loss']*x['batch_size'] for x in outputs])/total\n",
    "        avg_acc = sum([x['acc']*x['batch_size'] for x in outputs])/total\n",
    "      \n",
    "        if mode=='val':\n",
    "            print(f\"Epoch {self.current_epoch}:\\t Validation acc: {avg_acc:.4f}\\t Validation loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return {\"epoch_val_loss\": avg_loss, \"epoch_val_acc\": avg_acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "      \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "\n",
    "        outputs = self.validation_epoch_end(outputs, mode=\"test\")\n",
    "        return {\"test_loss\": outputs['epoch_val_loss'], \n",
    "                \"test_acc\": outputs['epoch_val_acc']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ao6nxS--YTUc"
   },
   "outputs": [],
   "source": [
    "ds = SSTDataModuleLSTM()\n",
    "ds.setup(min_freq=3)\n",
    "embed_mat = ds.embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "EdI046xvDEzi",
    "outputId": "acaafee0-1ea1-4844-8fd8-3df7c3a7b6f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name          | Type       | Params\n",
      "---------------------------------------------\n",
      "0 | embedding     | Embedding  | 4 M   \n",
      "1 | read_lstm     | LSTM       | 722 K \n",
      "2 | write_lstm    | LSTM       | 1 M   \n",
      "3 | compose_layer | Linear     | 360 K \n",
      "4 | fc            | Sequential | 77 K  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\t Validation acc: 0.7913\t Validation loss: 0.4415\n",
      "Epoch 0:\t Train loss: 0.3872\n",
      "Epoch 1:\t Validation acc: 0.8154\t Validation loss: 0.4219\n",
      "Epoch 1:\t Train loss: 0.3096\n",
      "Epoch 2:\t Validation acc: 0.7982\t Validation loss: 0.4165\n",
      "Epoch 2:\t Train loss: 0.2764\n",
      "Epoch 3:\t Validation acc: 0.8005\t Validation loss: 0.4323\n",
      "Epoch 3:\t Train loss: 0.2553\n",
      "Epoch 4:\t Validation acc: 0.7970\t Validation loss: 0.4522\n",
      "Epoch 4:\t Train loss: 0.2356\n",
      "Epoch 5:\t Validation acc: 0.7959\t Validation loss: 0.4284\n",
      "Epoch 5:\t Train loss: 0.2171\n",
      "Epoch 6:\t Validation acc: 0.7867\t Validation loss: 0.4467\n",
      "Epoch 6:\t Train loss: 0.2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\t Validation acc: 0.7936\t Validation loss: 0.4430\n",
      "Epoch 7:\t Train loss: 0.1883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NSE(len(ds.encoding), embed_mat=embed_mat)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='epoch_val_loss',\n",
    "   min_delta=0.0001,\n",
    "   patience=5,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./checkpoints/'+'{epoch}',\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='epoch_val_loss',\n",
    "    mode='min',\n",
    "    prefix=model.__class__.__name__+\"_\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, progress_bar_refresh_rate=0, max_epochs=30, \n",
    "                     num_sanity_val_steps=0, \n",
    "                     early_stop_callback=early_stop_callback,\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "GMu7hrcWDNt9",
    "outputId": "4896e618-68ff-468a-8f98-4047052b8c19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.8193, device='cuda:0'),\n",
      " 'test_loss': tensor(0.3975, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.test(model, \n",
    "                            ds.test_dataloader(), \n",
    "                            ckpt_path=checkpoint_callback.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentiment-analysis-sst",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
